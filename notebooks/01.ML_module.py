# -*- coding: utf-8 -*-
"""6sp _Ishta _Yojith_Mahela.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Aja6rO72weXorZOcOMYkmaIpWI1t_1ER
"""
"""
pip install nltk
pip install imblearn

has done in the jupiter Notebook

"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#read the data set with the name "Sinhala_news_articles.csv" and store it in a variable df

df = pd.read_csv("/Sinhala_news_articles.csv")
df.head()

df.shape

df.Label.value_counts()

df.Label.value_counts().plot.pie(autopct='%.2f')

#Split our data into features(X) and labels(y)
X=df.drop('Label',axis=1)
y=df['Label']

"""# Random undersampling due to class imbalance"""

from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(sampling_strategy='auto') # Numerical value
X_res, y_res = rus.fit_resample(X, y)
ax = y_res.value_counts().plot.pie(autopct='%.2f')
_ = ax.set_title("Under-sampling")

X_res

y_res

y_res.value_counts()

df.head()

X_res['Label'] = y_res
X_res

X_res['Label_num'] = X_res['Label'].map({'Business':0 , 'International':1 , 'Sport':2})
X_res.head()

bdf =X_res #balanced data frame
bdf

import re
import string

bdf["Title"].head(5)

"""Remove punctuations"""

string.punctuation

def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, '')
    return text

bdf["Title"] = bdf["Title"].apply(remove_punctuations)

bdf

"""remove numbers"""

bdf["Title"] = bdf['Title'].str.replace('\d+', '', regex=True)

bdf

"""remove stopwords"""



import nltk

with open('stop words.txt', 'r') as file:
    sw = file.read().splitlines()

sw

bdf["Title"] = bdf["Title"].apply(lambda x: " ".join(x for x in x.split() if x not in sw))

bdf

with open('suffixes_list.txt', 'r') as file:
    sw = file.read().splitlines()


bdf["Title"] = bdf["Title"].apply(lambda x: " ".join(x for x in x.split() if x not in sw))

bdf

"""stemming"""

with open('stem_dictionary.txt', 'r') as file:
    sw = file.read().splitlines()

sw

wd = []
stem = []
Dict = {}
for twords in sw:
  word = twords.split('\t')
  Dict[word[0]] = word[1]
  #stem.append(word[1])
  #wd.append(word[0])

Dict

def add_stem(text):
    for word in Dict:
        text = text.replace(word, Dict[word])
    return text

bdf["Title"] = bdf["Title"].apply(add_stem)

bdf

from collections import Counter
vocab = Counter()

for sentence in bdf['Title']:
    vocab.update(sentence.split())

vocab

len(vocab)

tokens = [key for key in vocab if vocab[key] > 3]

len(tokens)

def save_vocabulary(lines, filename):
    data = '\n'.join(lines)
    file = open(filename, 'w', encoding="utf-8")
    file.write(data)
    file.close()

save_vocabulary(tokens, 'vocabulary.txt')

"""divide dataset"""

X = bdf['Title']
y = bdf['Label_num']



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

X_train

y_train

def vectorizer(ds, vocabulary):
    vectorized_lst = []

    for sentence in ds:
        sentence_lst = np.zeros(len(vocabulary))

        for i in range(len(vocabulary)):
            if vocabulary[i] in sentence.split():
                sentence_lst[i] = 1

        vectorized_lst.append(sentence_lst)

    vectorized_lst_new = np.asarray(vectorized_lst, dtype=np.float32)

    return vectorized_lst_new

import numpy as np
vectorized_x_train = vectorizer(X_train, tokens)

vectorized_x_test = vectorizer(X_test, tokens)

vectorized_x_train

y_train

vectorized_x_test

y_test

y_train.value_counts()

import matplotlib.pyplot as plt
plt.pie(np.array([y_train.value_counts()[0], y_train.value_counts()[1],y_train.value_counts()[2]]), labels=['Business','International', 'Sport'])
plt.show()

"""Model Training and Evaluation"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

def training_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    pr = round(precision_score(y_act, y_pred), 3)
    rec = round(recall_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Training Scores:\n\tAccuracy = {acc}\n\tPrecision = {pr}\n\tRecall = {rec}\n\tF1-Score = {f1}')

def validation_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    pr = round(precision_score(y_act, y_pred), 3)
    rec = round(recall_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Testing Scores:\n\tAccuracy = {acc}\n\tPrecision = {pr}\n\tRecall = {rec}\n\tF1-Score = {f1}')

y_train.value_counts()

y_test.value_counts()

vectorized_x_test

vectorized_x_train

"""Model training and evaluation"""

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Create an instance of Logistic Regression
classifier = LogisticRegression()

# Create an instance of OneVsRestClassifier with Logistic Regression as the base classifier
ovr_classifier = OneVsRestClassifier(classifier)

# Train the OvR classifier
ovr_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = ovr_classifier.predict(X_test)

# Evaluate the classifier
print(classification_report(y_test, y_pred))

from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# Create an instance of Random Forest Classifier
classifier = RandomForestClassifier()

# Train the classifier
classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = classifier.predict(X_test)

# Evaluate the classifier
print(classification_report(y_test, y_pred))

from sklearn.datasets import load_iris
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Create an instance of Support Vector Machine (SVM) Classifier
classifier = SVC()

# Train the classifier
classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = classifier.predict(X_test)

# Evaluate the classifier
print(classification_report(y_test, y_pred))

from sklearn.datasets import load_iris
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


# Create an instance of Multinomial Naive Bayes Classifier
classifier = MultinomialNB()

# Train the classifier
classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = classifier.predict(X_test)

# Evaluate the classifier
print(classification_report(y_test, y_pred))